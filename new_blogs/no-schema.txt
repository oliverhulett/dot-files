Beyond NoSQL, NoSchema.

A schema says that we know what our data should look like.  We know what questions we want to answer and what queries we're going to have to run to answer those questions.
I would argue that in many cases that doesn't hold at Optiver.  At the very least as the time progresses and our trading environment and methods change the questions we want to answer change.  A schema forces us to update the data producing applications before we can answer the new questions.
Realistically we actually spend a lot of time massaging our current schema to answer the new questions, because the lead time for producing data to match a new schema is too slow.  So we find ourselves in a situation where we're massaging data into a schema on the producing side, and then massaging it again before we can run the reports or answer the questions we want to answer.
If we're going to be massaging data at the consuming side anyway, why bother massaging it into a schema at all at the producing side, when we should be spending our efforts trading instead?



Annectode:
I wanted to add some usage statistics gathering to build.py.  Turns out Gareth Edwards had already started doing so.  His approach was to encode usage in a graphite path.
Because he was trying to re-use an existing schema, he has hit on a solution that gathers much less information, is space inefficient, and still requires massaging on the consuming side to generate a useful report.

I would have had build tool gather:
 - usage stats on user, machine, cmdline, git remote, git branch, git hash, task(s), success (global & per task), error code, execution summary, start time, end time, cpu/memory/disk usage stats, docker image details, host os details, invoke version, invocations version
 - report some, json dump all
 - json dump to usage file, periodic upload (somewhere), accumulate and generate reports
 - report on frequency by projet, flags/tasks, style (bdp vs invoke), versions (invocations and invoke), success, error code, execution time

Instead we got:
+digraph {
+    builds -> repository;
+    repository -> branch;
+    branch -> ci_build;
+    branch -> not_ci_build;
+    ci_build -> username;
+    not_ci_build -> username;
+    username -> operating_system;
+    operating_system -> number_of_threads;
+    number_of_threads -> no_ccache;
+    number_of_threads -> ccache;
+    no_ccache -> no_distcc;
+    ccache -> no_distcc;
+    ccache -> distcc_optiverinternal;
+    ccache -> distcc_aws;
+    no_distcc -> build_type;
+    distcc_optiverinternal -> build_type;
+    distcc_aws -> build_type;
+    build_type -> build_step;
+}
as a flat structure in a graphite dot path.
Any combination that is actually used creates a block of storage on the graphite server, data is only kept for so long.
The natural report that comes out of this is when each combination was run and how long it took.  We loose any combinations that happen in the same second as another combination.  We need to further massage the data to get frequency of any individual component.  For example the question, how often is the optiver-internal distcc farm used?


Proposal:
Data producers just write files/stream of events.
 - (If file) each line is an event.
Events are stored directly and easily parsable (e.g. json)  (Current log files can probably be fairly easily massaged into events.)
Post processes can:
 - filter events
 - move/copy events
 - create new events
   - e.g. combine events, add colo details to events, etc.
Data consuming then becomes a problem of discovering events and combining or massaging them into a form that is useful to the question you're trying to answer:
 - duck type them; if it is for the time period you're interested in, and was generated by the application(s) you're interested in, and has the fields you're interested in, it's the data you want.
It'll be chaos, what's to stop people changing log lines and breaking all the reports?
 - If you've got a long running report in production that people are using to make decisions, test it like everything else in production.  Including ensuring that new releases of data producers give you the data you need.
 - If a new data producer version legit changes a log line that you were using, can you re-create the event by combining/massaging other events?  Install a post process to sit between the new data producer log line and your report.

Need to manage:
 - Keeping events
 - Moving events
 - Requesting events
 - Running post processes
 - Reporting post process utilisation (expiring post processes)

Are there any off-the-shelf products that get close to what we want or that we can glue together to get what we want?
 - mongodb?